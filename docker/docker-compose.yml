version: "3.9"

services:
  model-downloader:
    image: python:3.11-slim
    container_name: qwen3-downloader
    environment:
      HF_TOKEN: ${HF_TOKEN:-""}
    volumes:
      - ./models:/models
      - ./scripts:/scripts
    command: >
      bash -c "
        pip install --no-cache-dir huggingface_hub>=0.23 &&
        python /scripts/download_model.py &&
        echo 'Model ready at /models/model.gguf'
      "
  llama-server:
    image: ghcr.io/ggerganov/llama.cpp:server-cuda
    container_name: qwen3-llamacpp
    depends_on:
      - model-downloader
    volumes:
      - ./models:/models
    ports:
      - "8080:8080"
    environment:
      - HF_TOKEN=${HF_TOKEN:-""}
    command:
      [
        "-m", "/models/model.gguf",
        "--host", "0.0.0.0",
        "--port", "8080",
        "-c", "8192",
        "-ngl", "999",
        "-t", "16",
        "--parallel", "8",
        "--batch-size", "512",
        "--api-key", "sk-dev-qwen3"
      ]
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              driver: nvidia
    restart: unless-stopped
